# Predicting NFL Outcomes Using XGBoost in R

<a href="https://drive.google.com/file/d/1av1URyOtG7YHQVQwX4XdyxVFxox35mDl/view">Link to Deck</a>

## Introduction
Given that the NFL, NFL journalism, and NFL sports betting are all multi-billion dollar industries, being able to predict the outcomes of NFL games is of great real-life importance.  Every week, oddsmakers and sports media outlets try to use their models to predict betting odds, spreads, and game results.  Unfortunately, entertainment firms and seasoned bettors alike regularly leverage notoriously inaccurate predictions, costing themselves countless viewers and oodles of hard-earned cash.  To address this problem, I intend to use a machine learning algorithm to provide accurate predictions of NFL winners. This model will be massively beneficial for bettors, granting an edge over oddsmakers in the money line, spread, or fantasy football gambling; and it will also help the lads on shows like First Take to provide accurate takes and attract views.  From a more casual standpoint, my model will also help fans better root for their favorite teams.  By including a variable importance component, my model will help fans understand the keys to NFL victory and root for developments tailored toward these ends.  For example, if my model weighs passing success over rushing success, fans will know to celebrate the signing of a new WR over that of a new RB. In this manner, my model will be just as useful to the average fan as it will be to a professional bettor or career journalist.
To create this model, I had to collect several relevant statistics, including average home team run EPA, average home team pass EPA, average away team run EPA, and average away team pass EPA.  EPA stands for expected points added, and it is an effective way to measure success in rushing and passing plays.  Along with defensive metrics that added further predictive firepower to my set of inputs, I was able to output an XGBoost model achieving an accuracy of 60% and an AUC of 0.648. 

## Related Work 
Given the money involved, there have been other studies conducted similar to the one I am planning to take up. One such report, “Rush versus Pass: Modeling the NFL” uses sports ranking systems considering rushing and passing inputs to predict NFL games. In this report, the authors conclude that passing metrics are a better predictor than running metrics in terms of predicting game scores. Another study published by the United States Sports Academy explores whether controlling the running or passing game is the key to NFL victories. This study is interesting since the common perception is that running the ball and controlling the clock (as the clock keeps ticking on running plays) is a better indicator of NFL wins. The findings of this study, however, dispel this myth, pointing out that teams that run the ball excessively generally happen to already be winning their games, which in itself is the best predictor of victory (obviously).  Through studies such as these, I gained confidence in the relevance of my project and was also energized to improve upon existing predictions using the 15-week lagging window technique.

## Data Description and Pre-Processing
The primary data source for my project was a dataset named nfl_pbp_1999_2020, which included 731,978 rows and 370 columns. Each of these rows corresponded to a unique NFL regular season play run between the 1999 and 2020 seasons; and the columns included information on the downs, distances, players involved, outcomes, and expected points added for each play. After acquiring and loading this dataset into R, I began to perform a series of rigorous manipulations to produce a final data frame directly tailored to the goal of predicting NFL outcomes.
Extracting Unique Games: After loading the massive pbp_use dataset into the global environment, I extracted all unique games played between 1999 and 2020, dropped all games played after week 15 of each season (since many teams rest their players during these late-season contests), and tagged the remaining games with a weekly index ranging from 1 to 345.  Following these basic alterations, I then employed the epa_calculator_opp() and epa_calculator_def() functions written by Professor Barron to aggregate rushing, passing, and defensive EPA stats for home and away teams over lagging 15-week windows, quantifying how each team was performing entering a given game.  After binding these crucial statistical columns to the data frame of unique games, I saved the resulting table as mod_dat and moved on to adding my response variable. 
Adding the Response Variable: Since I wanted to predict which teams would win NFL games, I chose to make a binary winner column for my response variable, mapping 0 to a loss and 1 to a win.  Notably, there were a few stray ties included in mod_dat; but these were irrelevant outliers that were easily filtered out.
Adding Elo Ratings: As an additional powerful predictor, I chose to add columns tracking the Elo ratings of home and away teams to my analysis.  Since Elo ratings are an effective way to quantifiably rank competitors, they helped shed light on the relative strength of each team in each game.  To calculate these Elo ratings, I utilized the elo.calc() function from the elo library, using 1500 as the base rating for each team and iteratively adjusting this value as wins and losses accrued.  As a result of this looped process, I saved a lengthy vector of weekly Elo ratings for the home teams and another for away teams. 
As the above visualizations indicate, these Elo values were crucial for ultimately determining the outcomes of NFL games, as teams with high Elo ratings facing opponents with lower Elo ratings had the best chances of winning.
Increasing the Sample Size: To double my sample size, I chose to break each game into two rows: one row would focus on the home team facing the opposing away team, and the other would focus on the away team facing the opposing home team.  In essence, this transformation allowed us to present each row simply as one team facing an opponent team, relegating the home/away distinction to a different binary variable (mapping 0 to away and 1 to home).  To carry out this massive manipulation, I relied on Professor Barron’s methodology and began by extracting the original home team stats from mod_dat2 and the original away team stats from mod_dat2.  I saved these resulting tables in the hstats and astats data frames, respectively; and I then created copies of each and appended the suffix “_opp” to their variable names.  Saving these copies as hstats_1 and astats_1, I now had the 4 data frames I needed to finish my transformation.  All I had to do was column-bind hstats and astats-1 together, column-bind astats and hstats_1 together, append the pre-saved Elo ratings to each data frame, and row-bind the two tables together in the use_data variable.
Adding Derived Variables: To further heighten my model’s predictive power, I added four interaction terms to use_data.  These four terms, run_int, pass_int, def_run_int, and def_pass_int, tracked the strength of each team’s offensive and defensive capabilities, mitigated or heightened by the opponent team’s corresponding response.  Along with these more basic interaction terms, I also created a number of complex predictors to help generate powerful score predictions.  For each row, I calculated the deviation of each stat from its corresponding column mean.  Then, with these standardized measures and their column means, I was able to compute the number of runs, the number of passes, average run EPA, and average pass EPA expected from each team each week (given the opponent’s defense); and I was also able to compute the number of runs, the number of passes, average run EPA, and average pass EPA expected from each opponent (given the team in question’s defense).  With these derived variables, I was then able to generate the total expected EPA for each team and opponent and output a rough score prediction.  Though this complex predictor did not appear to be the deciding factor in designating a game as a win or a loss, as evidenced by the relatively undifferentiated density plot on the right, I still viewed it as a relevant independent variable that could perhaps lend some predictive power throughout the modeling process.  After column-binding these derived variables to use_data, my final table had 9,734 rows and 87 columns.
Training and Test Data: To segment my data, I placed all games with an index value less than 300 in the training set and sent the remaining rows to the test set.  Though this method of splitting the data did not involve serious random sampling, it was an effective way to derive two datasets of game information that included enough games to be representative of NFL performance as a whole.  After executing this splitting strategy, the training table had 8,398 observations; and the test one had 1,336.  By dropping non-numerical columns and saving the two resulting data frames as matrices, I finally obtained the boosting-compatible training and test datasets needed to begin modeling.

## Methods
After preparing my data for use, my next step was to load it into a model.  For my model of focus, I decided to use XGBoost.  XGBoost models are exceptionally powerful machine learning algorithms that build decision trees sequentially, iteratively placing higher weights on misclassified samples to encourage future trees to capture their underlying characteristics.  This boosting approach leads to an incredible degree of accuracy, even on “difficult-to-classify” samples; and by combining the results of all previously grown trees to make final predictions, XGBoost generally results in better performance than random forests or bagging models.  Moreover, as a tree-based algorithm, XGBoost is also highly resistant to missing values, which were pervasive throughout my training and test sets.  Given these strengths, I determined that XGBoost would be the optimal machine learning option for my project.   With a massive dataset full of nulls and wacky NFL game results, XGBoost would provide the best chance of attaining a respectable degree of accuracy.  Despite these strengths, however, XGBoost is not without weaknesses.  The largest drawbacks to XGBoost are its computational inefficiency and tendency to overfit to outliers and over-weighted samples.  Though I simply had to bite the bullet in terms of the long training times and memory strain, I did take concrete measures to prevent overfitting.  Namely, I made sure to optimize the number of trees grown using cross-validation, preventing XGBoost from completely molding itself to the training dataset.  With this measure, I was confident to proceed with XGBoost and begin modeling.

## Results
Relevant Metrics: In the spirit of p-hacking prevention, I decided upon evaluation metrics before I began modeling.  I chose to focus on accuracy and AUC, as my model predicts the categorical winner of each game and thus has a binary response variable.  Since accuracy tracks the fraction of correct classifications and AUC reports the aggregated measure of performance across all classification thresholds, both metrics were crucial in judging model performance throughout the tuning process.
Parameter Tuning and Model Fit: To optimize the predictive power of my XGBoost model, I began by tuning the number of trees necessary for peak performance.  If I built too many trees, the model would erroneously overfit to the boosted samples; and if I had too few trees, the model would not fit enough to the underlying trends. To determine the happy medium, I employed 5-fold cross-validation, randomly splitting the training data for each new tree added into five subsets and fitting the model 5 times, each time leaving one subset out as a validation table.  By employing this statistical method, I could track the model’s performance on the five previously unseen validation sets for each new tree grown, calculating the resulting validation error and AUC and determining whether more or fewer trees would improve performance.  Using this logic, I created the graph on the right to visualize how the validation error changed based on the overall number of trees built.  With an absolute minimum error rate of 0.397 at 59 trees, I chose 60 as the value of the early_stopping_rounds hyperparameter, directing XGBoost to stop iterating at 60 if validation error was not significantly improving.  Having tuned the number of trees needed, I then turned to the max.depth and min_child_weight parameters, referring respectively to the number of partitions allowed in each tree and the minimum sum of weights required in each leaf.  To optimize the values of these hyperparameters, I created a grid of possible combinations and ran a 5-fold cross-validation on each one.  By utilizing heat maps of the highest validation AUC and lowest validation error rates outputted from each of these cross-validation tests (see appendix), I determined that a max.depth of 5 and a min_child_weight of 5 would optimize the validation AUC at 0.630 and the validation error at 0.390.  I used a similar process to tune the gamma parameter or the minimum decrease in the loss function necessary for each tree to generate another partition.  Though not involving a grid, this tuning process involved employing 5-fold cross-validation on several possible gamma values.  Following the execution of this loop, I selected the optimal gamma value of 0.1, which maximized the validation AUC at 0.634 and minimized the validation error rate at 0.387.
Given these modified hyperparameters, I chose to re-determine the right number of trees to grow to minimize the validation error rate.  Using the newly-optimized values of 5 for max.depth, 5 for min_child_weight, and 0.1 for gamma, I found that 73 trees minimized the validation error at 0.391.  With this output, I flexed early_stopping_rounds to 75 for the remaining tuning steps.
Moving on, I then tuned the subsample and colsample_bytree parameters, referring respectively to the fraction of training observations and the fraction of predictors to be randomly included in each tree.  Using the same process outlined for max.depth and min_child_weight, I chose 0.6 for subsample and 1 for colsample_bytree, resulting in a validation error rate of 0.392 and a sky-high validation AUC  of 0.633.  Though this error rate was only the third-best among the combinations tested, the outstanding AUC more than made up for the marginally worse error metric.  Finally, to tune the eta hyperparameter or the learning rate, I used a similar process to the one used to tune gamma.  Feeding several different possible eta values into cross-validation tests, I tracked how the validation error rate for each eta value changed with the number of trees grown.  Ultimately, I determined that an eta value of 0.05, with 109 trees, would minimize the validation error at 0.390.  With this last determination, I set eta at 0.05 and nrounds/early_stopping_rounds at 109 for my final model.
Model Output: Having completed the lengthy tuning process and chosen optimized hyperparameter values, I generated my final model.  This model yielded a training error rate of 0.343 and a training AUC of 0.724, and the test error was 0.404 (accuracy of 0.596) with an AUC of 0.648.  The confusion matrix and AUC plot for this analysis are provided on the right.
As this confusion matrix indicates, my model had a strong sensitivity of 0.834, enjoying a powerful ability to identify true positives.  With this high sensitivity, however, came a low recall of 0.592, reflecting my model’s tendency to over-classify observations as wins.  All in all, however, the AUC metric was still high and the model was still effective in predicting NFL outcomes.
Challenges: Despite the success of this presented XGBoost model, I ran into trouble early on in my project by initially trying to predict the quantitative point differential for each game as a means of determining winners.  Though this approach seemed logically sound, I had no idea how unreliable my point differential predictions would be.  After carrying out an extensive tuning process and applying my final model to the test data, I plotted the residuals in the density plot provided on the left.  With error terms regularly upwards of 15 points, I realized how inaccurate my score predictions were; and these faulty predictions were crippling my ability to determine NFL winners.  Given this disappointing realization, I pivoted to building the model described in this report; and I achieved much more favorable results.
			
## Discussion
Ultimately, I can see my model tends to predict NFL wins and losses with nearly 60% accuracy, meaning that my model will often beat a random guess between two teams. In gambling contexts, an accuracy of about 60% will allow for overall profit in the aggregate and the long-term if bets are placed over multiple games, assuming this model will supplement other methods of determining a winner. For example, if a gambler placed 16 bets (one on each game in an average in-season week) of $10 with a line of +140, knowing that 8 games are obvious calls, but the other 8 am slightly less certain, the expected return would be (.6 [proportion of correct bets] * 8 [number of unsure bets] * 10 [dollars per bet] * 1.4 [return multiplier]) + (10 [dollars per bet] * 1.4 [return multiplier] * 7 [correct bets out of 8 confident predictions]), equaling a profit of $165.20. Though modest, this return could be scaled up with larger bets, allowing for a larger net return in the long run. This scenario is also highly idealistic, with a sample of 16 games, due to the model’s low recall making it slightly more biased towards predicting wins; but if this process were repeated every week the returns should scale and the return on investment should begin to converge toward the model’s accuracy. Another potential application of the model’s findings is for sports analytics and talk shows. In the world of sports news and entertainment, accurately predicting wins and losses can heavily contribute to viewership and ratings. Although the final model can’t predict with perfect accuracy every single game, six out of ten games being predicted correctly would bolster a show’s reputation and brand. Being a respected source of sports commentary also makes for excellent product endorsement deals, especially with the male 18-60 segmented market demographic. Companies searching for high-conversion exposure to this demographic range include those selling razors, alcoholic beverages, hair loss products, and many more; and such firms would float their advertising dollars to entertainment outlets with the best predictions and most viewers.  Finally, looking at variable importance (above), I can see that Elo, Opponent Elo, and Location are the most important variables in the model. This is intuitive, given the fact that Elo functions as a power ranking for the teams; and “home field advantage” is often cited by players and coaches alike.  One interesting insight I can garner from the variable importance chart, however, is that run defense is significantly important in the model’s output after these three more important variables.  Apparently, the adage that “defense wins Super Bowls,” especially when it comes to stuffing the run, remains true; and coaches would do well to remember it.

## Conclusion and Future Work
In sum, although my model was not quite as accurate as I would have hoped, it provided insight into unpredictable games in which one might be otherwise biased towards trusting an inaccurate guess.
In the future, I would like to further analyze point spreads to allow for betting across viewership data, thus allowing them to predict which games are more valuable and produce a higher return on ad spend. Additionally, pairing the data with historical gambling data to compare win predictions with the money line odds would allow for a calculation of the most advantageous bets to place (those with the highest discrepancy/opportunity for arbitrage). Further, merging play-by-play data with time-stamped viewership data could prove valuable for advertisers, especially if I were able to predict viewership dynamically throughout the game. This model would provide advertisers with valuable insight into when they should purchase ad time with regards to which games to select and which moments within the games will likely see the highest engagement leading to the greatest impressions, conversions, and ROAS.






